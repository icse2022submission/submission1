{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6aeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516af928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data: food.csv | investing.csv\n",
    "data = pd.read_csv('food.csv')\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "data.date = pd.to_datetime(data.date)\n",
    "\n",
    "# get preprocessed + original reviews for analysis and intrinsic evaluation\n",
    "fin_texts = [txt.split() for txt in data.loc[(data['for_analysis']) & (data['content_processed'].notnull()), 'content_processed'].values.tolist()]\n",
    "original_texts = data.loc[(data['for_analysis']) & (data['content_processed'].notnull()), 'content'].values.tolist()\n",
    "corpus_size = sum([len(doc) for doc in fin_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wikipedia corpus for extrinsic evaluation\n",
    "wiki_corpus_size = 2531105734 # precomputed\n",
    "from scipy import sparse\n",
    "wiki_matrix = sparse.load_npz(\"D:\\\\wiki\\\\wiki_dump_binary_matrix.npz\")\n",
    "with open(\"D:\\\\wiki\\\\features.txt\", 'r', encoding='utf-8') as f:\n",
    "    wiki_features = f.readlines()\n",
    "wiki_features = [feature.strip() for feature in wiki_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad92bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a hybrid tfidf model with a given threshold\n",
    "# higher thresholds give greater weight to longer texts\n",
    "THRESH = 6\n",
    "\n",
    "import hybridtfidf\n",
    "\n",
    "h = hybridtfidf.HybridTfidf(threshold=THRESH)\n",
    "document_vectors = h.fit_transform(fin_texts)\n",
    "document_weights = h.transform_to_weights(fin_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate top-k summaries\n",
    "# lower similarity thresh. will produce more distinct seeds\n",
    "K = 50\n",
    "SIMILARITY = 0.2\n",
    "\n",
    "from hybridtfidf import utils\n",
    "most_significant = utils.select_salient_posts(document_vectors, document_weights, k=K, similarity_threshold=SIMILARITY)\n",
    "\n",
    "for i in most_significant:\n",
    "    print(original_texts[i]) \n",
    "    print(fin_texts[i]) \n",
    "    print()\n",
    "del h\n",
    "del document_vectors\n",
    "del document_weights\n",
    "\n",
    "keywords = [fin_texts[i] for i in most_significant] \n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main cell\n",
    "# we use R's implementation of keyATM so conversion to Python's objects is necessary\n",
    "# converting data\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, StrVector, ListVector, pandas2ri\n",
    "\n",
    "#pandas2ri.deactivate()\n",
    "pandas2ri.activate()\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "quanteda = importr('quanteda')\n",
    "# keyATM package has to be installed first\n",
    "# https://cran.r-project.org/web/packages/keyATM/index.html\n",
    "keyATM = importr('keyATM')\n",
    "\n",
    "# next two functions below perform optimized NPMI calculation over intrinsic (corpus)\n",
    "# and extrinsic (wikipedia) datasets\n",
    "def calculate_npmi_corpus_keywords(keywords):\n",
    "    keywords_counts = {}\n",
    "    ret = []\n",
    "    # generate combinations\n",
    "    comb = list(combinations(keywords, 2))\n",
    "        \n",
    "    for i, (w1, w2) in enumerate(comb):\n",
    "        if w1 not in keywords_counts:\n",
    "            keywords_counts[w1] = sum([1 for doc in fin_texts for word in doc if w1 in doc])\n",
    "        if w2 not in keywords_counts:\n",
    "            keywords_counts[w2] = sum([1 for doc in fin_texts for word in doc if w2 in doc])\n",
    "            \n",
    "        # count both\n",
    "        p_w1_w2 = sum([1 for doc in fin_texts for word in doc if w1 in doc and w2 in doc])   \n",
    "        \n",
    "        if p_w1_w2 == 0: # they never occur together\n",
    "            res = -1.0\n",
    "        else:\n",
    "            p_w1 = keywords_counts[w1]\n",
    "            p_w2 = keywords_counts[w2]\n",
    "\n",
    "            p_w1 /= corpus_size\n",
    "            p_w2 /= corpus_size\n",
    "            p_w1_w2 /= corpus_size\n",
    "            num = math.log(p_w1_w2 / (p_w1 * p_w2))\n",
    "            res = num / -math.log(p_w1_w2)\n",
    "        ret.append((w1, w2, res))\n",
    "    return ret\n",
    "\n",
    "# memoing\n",
    "keywords_columns = {}\n",
    "keywords_sums = {}\n",
    "def calculate_npmi_wiki_keywords(keywords):\n",
    "    ret = []\n",
    "    keywords_updated = []\n",
    "    for keyword in keywords:\n",
    "        if keyword not in keywords_columns:\n",
    "            try:\n",
    "                w_id = wiki_features.index(keyword)\n",
    "            except:\n",
    "                continue\n",
    "            keywords_updated.append(keyword)\n",
    "            w_arr = wiki_matrix[:,w_id].A.flatten()\n",
    "            p_w = w_arr.sum()\n",
    "            keywords_columns[keyword] = w_arr\n",
    "            keywords_sums[keyword] = p_w\n",
    "        else:\n",
    "            keywords_updated.append(keyword)\n",
    "    # generate combinations\n",
    "    comb = list(combinations(keywords_updated, 2))\n",
    "        \n",
    "    for i, (w1, w2) in enumerate(comb):\n",
    "        w1_arr = keywords_columns[w1]\n",
    "        w2_arr = keywords_columns[w2]\n",
    "        p_w1_w2 = ((w1_arr == 1) & (w1_arr == w2_arr)).sum() \n",
    "        \n",
    "        if p_w1_w2 == 0: # they never occur together\n",
    "            res = -1.0\n",
    "        else:\n",
    "            p_w1 = keywords_sums[w1]\n",
    "            p_w2 = keywords_sums[w2]\n",
    "\n",
    "            p_w1 /= wiki_corpus_size\n",
    "            p_w2 /= wiki_corpus_size\n",
    "            p_w1_w2 /= wiki_corpus_size\n",
    "            num = math.log(p_w1_w2 / (p_w1 * p_w2))\n",
    "            res = num / -math.log(p_w1_w2)\n",
    "        ret.append((w1, w2, res))\n",
    "    return ret\n",
    "\n",
    "# evaluates the topic (top-10 words) using intrinsic and extrinsic evaluation\n",
    "def evaluate_topic_npmi(keywords):\n",
    "    topic_npmis = calculate_npmi_wiki_keywords(keywords)\n",
    "    topic_npmis_corpus = calculate_npmi_corpus_keywords(keywords)\n",
    "\n",
    "    return np.mean([npmi for _,_, npmi in topic_npmis]), np.mean([npmi for _,_, npmi in topic_npmis_corpus])\n",
    "\n",
    "# main function\n",
    "def main_pipeline(data, keywords):\n",
    "    # convert data to keyATM format\n",
    "    vec = quanteda.tokens(data)\n",
    "    dfm = quanteda.dfm(vec)\n",
    "    keyATM_docs = keyATM.keyATM_read(texts=dfm)\n",
    "    # prepare keywords\n",
    "    keyATM_keywords = ListVector([(\"T\" + str(i), StrVector(list(set(keyword_set)))) for i,keyword_set in enumerate(keywords)])\n",
    "    # train\n",
    "    scores = []\n",
    "    out = keyATM.keyATM(docs=keyATM_docs, no_keyword_topics=0, keywords=keyATM_keywords, model='base')\n",
    "    KEY_topic_word = pd.DataFrame(out.rx('phi')[0], columns=out.rx('vocab')[0])\n",
    "    for i,tname in zip(KEY_topic_word.index, r.colnames(keyATM.top_docs(out))):\n",
    "        topic_words = KEY_topic_word.iloc[i,:].sort_values(ascending=False).index[:10]\n",
    "        score_wiki, score_corpus = evaluate_topic_npmi(topic_words)\n",
    "        scores.append((score_wiki, score_corpus))\n",
    "\n",
    "    return out, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_scores_npmi_wiki = []\n",
    "all_scores_npmi_corpus = []\n",
    "# train keyATM for K={10,20,...,100}\n",
    "for i in range(10, 110, 10):\n",
    "    out,scores = main_pipeline(fin_texts, keywords[:i])\n",
    "    all_scores_npmi_wiki.append([score[0] for score in scores])\n",
    "    all_scores_npmi_corpus.append([score[1] for score in scores])\n",
    "    # save on each K\n",
    "    with open(\"scores\\\\npmi_wiki_0.txt\", 'wb') as f:\n",
    "        pickle.dump(all_scores_npmi_wiki, f)\n",
    "    with open(\"scores\\\\npmi_corpus_0.txt\", 'wb') as f:\n",
    "        pickle.dump(all_scores_npmi_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keyATM configurations (keyATM-5, -15, -25)\n",
    "# thresh defines the lower percentile\n",
    "# returns new keywords\n",
    "# keyATM can be re-trained using the main_pipeline with new keywords\n",
    "def remove_low_score(keywords, thresh=5):\n",
    "    topic_npmis = []\n",
    "    ret = []\n",
    "    for keyw_set in keywords:\n",
    "        new_keyw = []\n",
    "        npmis = calculate_npmi_corpus_keywords(keyw_set)\n",
    "        scores = {}\n",
    "        for w1, w2, score in npmis:\n",
    "            if w1 not in scores:\n",
    "                scores[w1] = []\n",
    "            if w2 not in scores:\n",
    "                scores[w2] = []\n",
    "            scores[w1].append(score)\n",
    "            scores[w2].append(score)\n",
    "        return scores\n",
    "        try:\n",
    "            scores = {w:np.mean(score) for w, score in scores.items()}\n",
    "            t = np.percentile(list(scores.values()), thresh)\n",
    "        except:\n",
    "            ret.append(keyw_set)\n",
    "            continue\n",
    "        for k,v in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            if v < t:\n",
    "                print(f\"REMOVED: {k} {v}\")\n",
    "            else:\n",
    "                new_keyw.append(k)\n",
    "            print(k, v)\n",
    "        ret.append(new_keyw)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA baseline\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(fin_texts)\n",
    "# Create Corpus\n",
    "texts = fin_texts\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_scores_npmi_wiki = []\n",
    "lda_scores_npmi_corpus = []\n",
    "for i in range(10, 110, 10):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=i, \n",
    "                                       chunksize=100,\n",
    "                                       per_word_topics=True,\n",
    "                                       iterations=1500)\n",
    "    LDA_doc_topic = pd.DataFrame([{pair[0]:pair[1] for pair in elem} for elem in lda_model.get_document_topics(corpus)], columns=range(0,16))\n",
    "    LDA_topic_word = pd.DataFrame(lda_model.get_topics(), columns=[it[1] for it in id2word.items()])\n",
    "    \n",
    "    temp_wiki = []\n",
    "    temp_corpus = []\n",
    "    for j in LDA_topic_word.index:\n",
    "        keyw = list(LDA_topic_word.iloc[j,:].sort_values(ascending=False).index[:10])\n",
    "        score_wiki, score_corpus = evaluate_topic_npmi(keyw)\n",
    "        temp_wiki.append(score_wiki)\n",
    "        temp_corpus.append(score_corpus)\n",
    "    lda_scores_npmi_wiki.append(temp_wiki)\n",
    "    lda_scores_npmi_corpus.append(temp_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
